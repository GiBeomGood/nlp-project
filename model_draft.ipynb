{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaModel,\n",
    "    RobertaConfig,\n",
    "    RobertaForSequenceClassification,\n",
    ")\n",
    "import torch.nn.functional as F  # noqa: N812\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 768,\n",
    "        num_heads: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        weight_kind: str = \"softmax\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layer_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.layer_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.layer_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.layer_output = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        if weight_kind == \"softmax\":\n",
    "            self.get_weight = nn.Softmax(dim=3)\n",
    "        elif weight_kind == \"tanh\":\n",
    "            self.get_weight = nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid value of `weight_kind`: {weight_kind}\")\n",
    "        return\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        query = self.layer_q(query)\n",
    "        key = self.layer_k(key)\n",
    "        value = self.layer_v(value)\n",
    "\n",
    "        query = query.view(-1, query.size(1), self.num_heads, self.head_dim)\n",
    "        key = key.view(-1, key.size(1), self.num_heads, self.head_dim)\n",
    "        value = value.view_as(key)\n",
    "\n",
    "        query = query.permute(0, 2, 1, 3).contiguous()  # (-1 x num_heads x T1 x d_h)\n",
    "        key = key.permute(0, 2, 3, 1).contiguous()  # (-1 x num_heads x d_h x T2)\n",
    "        value = value.permute(0, 2, 1, 3).contiguous()  # (-1 x num_heads x T2 x d_h)\n",
    "\n",
    "        attention = query @ key  # (-1 x num_heads x T1 x T2)\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask, -1e10)  # (-1 x num_heads x T1 x T2)\n",
    "        attention = self.dropout(self.get_weight(attention))  # (-1 x num_heads x T1 x T2)\n",
    "\n",
    "        output: Tensor\n",
    "        output = attention @ value  # (-1 x num_heads x T1 x d_h)\n",
    "        output = output.permute(0, 2, 1, 3).contiguous()  # (-1 x T1 x num_heads x d_h)\n",
    "        output = output.view(-1, output.size(1), self.embed_dim)  # (-1 x T1 x d)\n",
    "        output = self.layer_output(output)  # (-1 x T1 x d)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregater(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.attention_weight_layer = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_output = nn.Linear(embed_dim, embed_dim)\n",
    "        return\n",
    "\n",
    "    def forward(self, ts_embed: Tensor, text_embed: Tensor) -> Tensor:\n",
    "        # (-1 x 1 x d), (-1 x T x d)\n",
    "        output: Tensor\n",
    "        output = self.attention_weight_layer(text_embed)  # (-1 x T x d)\n",
    "        output = output.permute(0, 2, 1).contiguous()  # (-1 x d x T)\n",
    "        output = ts_embed @ output  # (-1 x 1 x T)\n",
    "\n",
    "        output = self.dropout(output.softmax(2))  # (-1 x 1 x T)\n",
    "        output = output @ text_embed  # (-1 x 1 x d)\n",
    "        output = self.layer_output(output)  # (-1 x 1 x d)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.roberta_config = RobertaConfig.from_pretrained(config.roberta.pretrained_path)\n",
    "        self.keyword_encoder = None  # TODO\n",
    "\n",
    "        self.ts_encoder = DLinear()  # TODO\n",
    "\n",
    "        self.aggregater = MultiHeadAttention(**config.aggregater)\n",
    "        self.layer_norm1 = nn.LayerNorm(None)  # TODO\n",
    "        self.context_keyword_attention = MultiHeadAttention(**config.context_keyword_attention)\n",
    "        self.layer_norm2 = nn.LayerNorm(None)  # TODO\n",
    "        self.final_ffn = nn.Sequential(  # (-1 x d_embed) -> (-1 x H)\n",
    "            nn.Linear(None, None),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(None, config.forecast_size),\n",
    "        )\n",
    "        return\n",
    "\n",
    "    def get_ouptut(\n",
    "        self,\n",
    "        input_ts: Tensor,\n",
    "        input_text: Tensor,\n",
    "        input_kw: Tensor,\n",
    "    ):\n",
    "        # `input_ts`: (-1 x T x d1) time series data (need to be normalized)\n",
    "        # `input_text`: (-1 x max_sents x d_embed) output of RobertaForSequenceClassification.roberta\n",
    "        # # good representation for sentiment analysis pretrained on twitter data\n",
    "        # # max_sents: set as 30, padding of sentences applied\n",
    "        # `input_kw`: (-1 x n_kw x d_embed) keyword data\n",
    "\n",
    "        # step 1: make representations for time series, text (already done), and keywords\n",
    "        rep_both = self.ts_encoder(input_ts)  # (-1 x d_embed)\n",
    "        rep_kw = self.keyword_encoder(input_kw)\n",
    "\n",
    "        # step 2: aggregate time series & text representations to make context vector\n",
    "        # # considers relationship between time series history and sentimental representation of text\n",
    "        temp = self.aggregater(rep_both, input_text, input_text, mask=None)  # (-1 x 1 x d_embed)\n",
    "        rep_both = self.layer_norm1(rep_both + temp)\n",
    "        # TODO: add keyword extraction\n",
    "\n",
    "        # step 3: aggregate context vector & keyword representations to make final output\n",
    "        output: Tensor\n",
    "        output = self.context_keyword_attention(rep_both, rep_kw, rep_kw, mask=None)  # (-1 x 1 x d_embed)\n",
    "        output = self.layer_norm2(rep_both + output)\n",
    "        # sum of keywords with attention weights \\in (-1, 1)\n",
    "\n",
    "        # step 4: feed forward network\n",
    "        output = output.squeeze(1)  # (-1 x d_embed)\n",
    "        output = self.final_ffn(output)  # (-1 x H)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def forward(self):\n",
    "        return\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self):\n",
    "        return\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate_batch(self):\n",
    "        return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
