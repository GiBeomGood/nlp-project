check_prop: 0.01
epochs: 200
device: cuda:4
# setting1: General Attention + Multihead Attention
# forecasting
data_name: stocknet
model_name: setting18
model_save_path: ./checkpoints/${data_name}/${model_name}.pt

ddp:
  kwargs:
    nprocs: 3
    join: true

wandb:
  do: false
  kwargs:
    project: nlp-project
    name: ${data_name}/${model_name}

dataset:
  window_size: 30
  forecast_size: 5
  min_sents: 2
  max_sents: 5
  min_words: 10
  max_words: 15
  train_prop: 0.7
  val_prop: 0.9

dataloader:
  train:
    batch_size: 128
    shuffle: true
    num_workers: 1
    pin_memory: true
  
  val:
    batch_size: 512
    shuffle: false
    num_workers: 1
    pin_memory: true
  
  test:
    batch_size: 512
    shuffle: false
    num_workers: 1
    pin_memory: true

early_stopping:
  val_key: val loss
  tolerance: 10
  higher_better: false

model:
  forecast_size: ${dataset.forecast_size}
  ts_feature_num: 2
  embed_dim: 768
  hidden_dim: 256
  do_kw_attention: false
  roberta_pretrained_path: cardiffnlp/twitter-roberta-base-sentiment-latest
  dropout: 0.3
  num_classes: 4

  loss_kwargs:
    weight: [0.35, 0.15, 0.15, 0.35]
  val_loss_kwargs:
    reduction: sum

  dlinear:
    seq_len: ${dataset.window_size}
    pred_len: ${dataset.forecast_size}
    individual: true
    enc_in: ${model.ts_feature_num}

  aggregater:
    # kind: general
    # query_dim: ${model.hidden_dim}
    # key_dim: ${model.hidden_dim}
    # dropout: ${model.dropout}
    # weight_kind: softmax
    kind: multihead
    embed_dim: ${model.hidden_dim}
    num_heads: 4
    dropout: ${model.dropout}
    weight_kind: softmax

  context_keyword_attention:
    # kind: multihead
    # embed_dim: ${model.hidden_dim}
    # num_heads: 4
    # dropout: ${model.dropout}
    # weight_kind: tanh
    kind: general
    query_dim: ${model.hidden_dim}
    key_dim: ${model.hidden_dim}
    dropout: ${model.dropout}
    weight_kind: tanh

optimizer:
  lr: 0.001