check_prop: 0.01
epochs: 200
device: cuda:4
# setting1: General Attention + Multihead Attention
# forecasting
data_name: stocknet
model_name: setting1
model_save_path: ./checkpoints/${data_name}/good/${model_name}.pt

ddp:
  kwargs:
    nprocs: 3
    join: true

wandb:
  do: false
  kwargs:
    project: nlp-project
    name: ${data_name}/${model_name}

dataset:
  window_size: 40
  forecast_size: 10
  min_sents: 2
  max_sents: 5
  min_words: 10
  max_words: 20
  train_prop: 0.7
  val_prop: 0.9

dataloader:
  train:
    batch_size: 128
    shuffle: true
    num_workers: 1
    pin_memory: true
  
  val:
    batch_size: 512
    shuffle: false
    num_workers: 1
    pin_memory: true
  
  test:
    batch_size: 512
    shuffle: false
    num_workers: 1
    pin_memory: true

early_stopping:
  val_key: val acc
  tolerance: 20
  higher_better: true

model:
  forecast_size: ${dataset.forecast_size}
  ts_feature_num: 2
  embed_dim: 768
  hidden_dim: 512
  roberta_pretrained_path: cardiffnlp/twitter-roberta-base-sentiment-latest
  dropout: 0.1
  num_classes: 4

  loss_kwargs: {}
  val_loss_kwargs:
    reduction: sum

  dlinear:
    seq_len: ${dataset.window_size}
    pred_len: ${dataset.forecast_size}
    individual: true
    enc_in: ${model.ts_feature_num}

  aggregater:
    kind: general
    query_dim: ${model.hidden_dim}
    key_dim: ${model.hidden_dim}
    dropout: ${model.dropout}
    weight_kind: tanh

  context_keyword_attention:
    kind: multihead
    embed_dim: ${model.hidden_dim}
    num_heads: 4
    dropout: ${model.dropout}
    weight_kind: softmax

optimizer:
  lr: 0.001